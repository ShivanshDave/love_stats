# 431 Materials posted after our final class of the year

## On Grades

- I posted course grades to SIS on 2019-12-15, and it appears some of you (at least) can see them now. Everyone received either an A or a B. There were more As than Bs, but a substantial number of each. Congratulations to you all.
- The minimum grade to receive an A in the course was 85% and for a B it was 70.0% to 84.9%. That same scale applied to all project grades and class participation grades. 
    - As it turns out, all but one student had a final grade below 83.2% or above 85.5%. 
- Receiving emails about grades is, frankly, my **least favorite part of the semester** when it happens, so let me specify a few things.
- You are welcome to send a note asking me for more information about your grade. Here is the complete set of information I will release to you. 
    - Your grade in the course was X%, based on 
    - a project grade of X%, a class participation grade of X%, homework of X% and a Quiz average of X%. 
- I will not break things down further than that, and I do not change grades in response to pleading, only in response to mistakes in my arithmetic. 
- In combining the four pieces above, I used the larger of 15% class participation, 30% homework, 25% quizzes, 30% project or 15% class participation, 25% homework, 30% quizzes, 30% project to [determine your final grade](https://thomaselove.github.io/2019-431-syllabus/deliverables-expectations-and-assessment.html#grading-breakdown) in the course. 
    - [As mentioned previously here](https://github.com/THOMASELOVE/2019-431/tree/master/QUIZZES#grading-at-the-end-of-the-semester), your quiz average takes 3*(your best quiz score) + 3*(your second best quiz score) + 2*(your worst quiz score) and divides by 8, and 
    - your homework average drops your lowest score on Homeworks B-I, [as was specified here](https://github.com/THOMASELOVE/2019-431/tree/master/HOMEWORK#grading). The only adjustment to the homework grade is for people who were excused from (as opposed to skipping) a homework. That only happened to three people.
    
Thank you, and have a nice break. Go do something good!

## NEW! 

- Dr. Love posted course grades to SIS on Sunday evening 2019-12-15, so that you should be able to view them through the Registrar's SIS no later than Tuesday 2019-12-17. 
- Grade on Quiz 3 are now available to you at [the usual place](http://bit.ly/431-2019-grades). The Quiz 3 answer sketch (final and revised to include grading details, password protected PDF) [is available for download here](https://github.com/THOMASELOVE/2019-431/blob/master/QUIZZES/QUIZ3/2019-431-quiz03-sketch_final_pw.pdf). The password is the same as is used for all class materials.
- 431-help is closed until 2020-01-13. Dr. Love will review email occasionally between now and then.
- Dr. Love's [brief comments after Day 1 of the Project Presentations are now available](https://github.com/THOMASELOVE/2019-431/blob/master/AFTER_CLASS/day1comments.md).

## Remaining Task for 431

Deadline | Remaining Task for 431
-------------------: | ------------------------------------------------------------------------------------------------
Wed 12-18 | Course Evaluation due at https://webapps.case.edu/courseevals/ (contact `help@case.edu` with questions).


## General Information

- There is a Cleveland R Users Group that holds meetups about once a month. The next one is on December 11, which may be tough timing for us, but it looks interesting. [Here are the details](https://www.meetup.com/Cleveland-UseR-Group/events/264059287/).
- Responses to **recent questions at 431-help** are found at the bottom of this document.
- The [432 home page for Spring 2020](https://github.com/THOMASELOVE/2020-432) now exists.
- So does the [500 home page for Spring 2020](https://github.com/THOMASELOVE/2020-500).
- Homework I grades are now available at http://bit.ly/431-2019-grades. 

## Materials for 432

- **When will the syllabus, etc. be posted?** I won't guarantee any time before our first class at 1 PM on 2020-01-14, but I'll do what I can to get something up meaningfully sooner. I'd be surprised if anything substantial was up before the New Year.
- I encourage you to read at least Chapters 1-5 of **The Art of Statistics: How to Learn from Data** by David Spiegelhalter over the break. We'll read the entire book through the Spring semester, but there's no reason not to read the whole thing in advance. Available for between $20 and $25 in hardcover, less as an e-book. Here's [the book's website](https://dspiegel29.github.io/ArtofStatistics/). I don't know yet whether this is the only book you'll have to buy for 432, or one of two books, but you do need to buy the Spiegelhalter.
- The other book you *might* need to buy and read is Jeff Leek's [How to Be a Modern Scientist](https://leanpub.com/modernscientist) which is great but getting a little bit out of date. So I'm not sure yet whether you need to buy this or whether I'll go another way.
- [Biostatistics for Biomedical Research](http://hbiostat.org/bbr/) (pdf file) by Frank E. Harrell, Jr. and James C. Slaughter is a nice source for a lot of things we'll be using in 432, plus a review and different spin on a lot of things from 431. It is freely available (and regularly updated) [online](http://hbiostat.org/bbr/).
    - Frank's video course on (some of) this material is [ongoing at YouTube](https://www.youtube.com/channel/UC-o_ZZ0tuFUYn8e8rf-QURA/videos).
- [Common statistical tests are linear models](https://lindeloev.github.io/tests-as-linear/) by Jonas Kristoffer Lindeløv.
- Dr. Love's Course Notes will be edited and rewritten over the break, he hopes. 
    - In the meantime, [here's the Spring 2019 version](https://thomaselove.github.io/2019-432-book/) which is posted here with no guarantees.

## Staying Sharp

The best thing to do is analyze some data. I can think of no better place to keep sharp than working on some of the [Tidy Tuesday projects](https://thomasmock.netlify.com/post/tidytuesday-a-weekly-social-data-project-in-r/) and I hope to spend some time on this myself over the break. There's a new data set every week!

- For instance, the [data for the week of 2019-12-03 involves parking violations in the city of Philadelphia](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-12-03).
- Here's the complete [list of 2019 data sets to date](https://github.com/rfordatascience/tidytuesday#2019).
- David Robinson's [video series continues on YouTube](https://www.youtube.com/user/safe4democracy/videos).

## What Is Dr. Love Reading?

- [Statistical Thinking for the 21st Century](http://statsthinking21.org/index.html) by Russell A. Poldrack
- Alberto Cairo's [How Charts Lie: Getting Smarter about Visual Information](https://www.amazon.com/How-Charts-Lie-Getting-Information/dp/1324001569) and [here's a nice 1-hour presentation Alberto gave in Detroit earlier this year](https://www.youtube.com/watch?v=Low28hx4wyk).
- I'm also hoping to find some more time to play with [rayshader](https://www.rayshader.com/).
- Lots of other things, many of which have nothing directly to do with data or data science or statistics, of course.

---------------------------

# Answers to Questions at 431-help

## posted 2019-12-07

A. What's the best way to get help while 431-help is out of commission from mid-December to mid-January?

- Google, probably, and the R Studio Community, and Stack Overflow. Dr. Love will respond when he can, but it won't be frequent regarding anything other than (a) very simple things or (b) very important things or (c) setup issues regarding the Spring classes (432 or 500.)

B. **Collinearity** Key points on collinearity off the top of my head?

1. Collinearity = strongly correlated predictors in a regression model.
2. Impacts of collinearity: (a) can't separate out what each individual predictor is doing, since most of the variation it explains is also explained by its highly correlated partner predictor(b) variance is inflated (e.g. the standard errors are inflated) for the regression coefficients, making the confidence intervals wider than they would be in the absence of collinearity(c) variable selection is tricky, because if you drop one predictor, suddenly the other may look very important, but if you first drop predictor A and predictor B then looks important, that doesn't guarantee that if you instead dropped B first, A would then look more or less important.
3. Serious collinearity is indicated by VIFs above 5. In such a case, the problems above apply. Best solution: adjust your choice of predictors accordingly (include one or the other, or combine them in some useful way if that still picks up most of the important variation in the model.)
4. Collinearity = not a violation of regression assumptions, like non-linearity or non-normality or non-constant variance  - more of a nuisance, really.

C. On **ANOVA vs. Kruskal-Wallis**

1. Should I be using a summary statistic, like skew1 to determine whether things are Normally distributed or not? No. You should be using plots. Only if the plot is borderline would I even think about a summary statistic instead.
2. Is the standard for Normality that I need to meet in an ANOVA (to avoid having to switch over to K-W) the same one I would use in thinking about a t test vs.a bootstrap? Essentially, yes, but if anything, the ANOVA Is MORE robust to modest violations of Normality assumptions than is the t test. You have to have some serious problems for me to move from ANOVA to K-W.

D. **Is it OK** if I change "this specific thing that I said I was going to do initially but now doesn't work well" in my analysis of Study A? How about the same question regarding Study B?

- Yes, absolutely.

## posted 2019-12-08

1. **Gini's Mean Difference** What is the `Gmd` in `Hmisc::describe()` output?

- That would Gini's mean difference. It's a measure of spread, like the IQR or standard deviation.From the Hmisc help file... This index is defined as the mean absolute difference between any two distinct elements of a vector. For a Bernoulli (binary) variable with proportion of ones equal to p and sample size n, Gini's mean difference is 2np(1-p)/(n-1). If you want more information on this index, and other forms of a mean absolute difference, check out the Wikipedia page. 

2. **On back-transformation** after (for example) a logarithm? 

- If you took the log of the outcome at the start, then when calculating errors in your test subset to assess the size of the prediction errors with things like MAPE and MSPE, you would want to use R to estimate "the actual value on the original scale" minus "the fitted value on the original scale", so you'd have something like `exp(actual) - exp(.fitted)` for each error, rather than, for example, the incorrect `exp(actual - .fitted)`. 
- When calculating residuals in the training sample to run residual plots, those should be the residuals that R generates on the log scale initially - so you just need to run `plot(modelname)` as always to see the plots and interpret them, regardless of whether or not you did a transformation.

3. **On back-transformation** and interpreting coefficients.

- When you've used a transformation (now, let's say an inverse) on your outcome, and you want to look at the effect of the coefficients, you have two options:

1. Interpret the coefficients in terms of the transformed scale. So that if your equation was 1/y = 234.9 - 22 x1 + 0.58 x2 - 4.5 x3, then you'd talk about the effect of x1 on 1/y holding the other two predictors constant, rather than the effect of x1 on y.

2. Use the model to help you with an interpretation for x1's effect on y using the original scale, as follows.
    1. Create two new observations, we'll call then subject A and subject B.
    2. A has the mean value of all predictorsB has the mean value of all predictors other than x1, and has (the mean value + 1) as their value for x1.
    3. Predict 1/y for A and 1/y for B using the model.Back transform out of those predictions to get predicted Y values for A and for B.
    4. Subtract B's predicted Y value minus A's predicted Y value to see the impact of changing x1 from its mean to "1 + the mean" on Y, while holding all other values constant at their means.
    5. Repeat the process to make a similar statement about x2, and then again for x3.

- I am OK with you using either approach (1 or 2) in your project. Done correctly, I prefer the second, but the first is, of course, much easier. I'm guessing you will be unsurprised to learn that R has a fast way of doing something like this, but since I haven't taught it yet, you're stuck with these two options for the project. I'll teach that in 432.
- What you absolutely cannot do (and be correct) is just take (for example) the inverse of the regression coefficients. That won't work at all.

4. On **Quiz 3, Question 4**

- In order to answer the question, you do not need to know the size of the large sample taken ten years ago. If you think you do need to know the size of that sample, you have moved in the wrong direction. If you like, you can assume it was very, very large. Everything you need to know about that sample is provided in the stem of the question.

5. **On submitting revised data**. "I have made some slight changes to my Study B tidied data set. Where do I submit this to make sure Dr. Love can access it?"

- With the portfolio.

6. **Study A Question 6 if Woolf test indicates trouble**. "For Study A analysis 6, we have done our Woolf test and the resulting p value is significant. In the notes, it only indicates that you continue to the Cochran-Mantel-Haenszel test if the value is not significant, but it never guides you to what the next step would be if you do get a significant p value in your Woolf Test. Would we still just do the CMH test? Or is there another analysis?"

- You have two options:

1. Do a separate 2x2 analysis for each stratum instead of the CMH.
2. Run the CMH test, then using the results of your Woolf Test, explain why the results of the CMH Test are likely invalid.

- Either is OK with me for the project.

7. Selecting a model based on **AIC/BIC if the values are negative**. 

- Suppose you have AIC values as follows: Model A: 23, Model B: 20. Then you'd pick Model B, because the AIC is smaller.
- Suppose the AIC values are negative, and Model A is -23 and Model B is -20. Then you'd pick Model A, because the AIC for Model A is more negative, and thus smaller than the Model B value.
- The same logic applies to BIC.

Thanks for the questions. Keep them coming!

## posted 2019-12-12

1. Regarding Question 4 on Quiz 3: Code 1 produces a 95% confidence interval, but Code 2 produces a 90% confidence interval. Why?

Code 1: 
```
prop.test(x = 12 + 1, n = 38 + 2) %>%
  tidy(conf.int = TRUE, conf.level = 0.90)
```

Code 2: 
```
prop.test(x = 12 + 1, n = 38 + 2, conf.level = 0.9) %>%
   tidy(conf.int = TRUE)
```

The answer is that `prop.test()` is not as friendly to `broom` (the package that gives us the `tidy()` function) as you appear to want it to be. In Code 1, in the first line, `prop.test()` has already selected a confidence level of 95% (by default.) In the second line, where `tidy()` appears, you try to tell it to change to 90%, but `prop.test()` doesn't store anything but the level it used when it did the other calculations in `prop.test()` so it just returns the initial level of 95% again. 

To produce a 90% confidence interval from a `prop.test()`, you must change what `prop.test()` produces initially, as in Code 2.

2. In Quiz 3, I have faked no output. Every bit of output and code I provided to you in the Quiz is real and comes from R code. That doesn't mean that every bit of output is relevant to answer the questions involved, but I faked nothing. If you try to replicate what I did using the code I provided, you should get an identical response.

